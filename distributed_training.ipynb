{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from config file\n",
    "config_path = \"config/training_config.yaml\"\n",
    "\n",
    "if Path(config_path).exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"✅ Loaded configuration from file\")\n",
    "else:\n",
    "    # Option 2: Define configuration here\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"type\": \"lora\",\n",
    "            \"lora\": {\n",
    "                \"rank\": 8,\n",
    "                \"alpha\": 16,\n",
    "                \"dropout\": 0.0,\n",
    "                \"scale\": 10.0,\n",
    "                \"num_layers\": 8\n",
    "            }\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"path\": \"data/sample_dataset.jsonl\",\n",
    "            \"output_dir\": \"data\",\n",
    "            \"validation_split\": 0.2,\n",
    "            \"max_seq_length\": 2048,\n",
    "            \"format_type\": \"chat\"\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 2,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"num_iterations\": 100,\n",
    "            \"val_batches\": 25,\n",
    "            \"steps_per_report\": 10,\n",
    "            \"steps_per_eval\": 50\n",
    "        },\n",
    "        \"checkpointing\": {\n",
    "            \"save_every\": 100,\n",
    "            \"output_dir\": \"adapters\"\n",
    "        },\n",
    "        \"distributed\": {\n",
    "            \"enabled\": False\n",
    "        }\n",
    "    }\n",
    "    print(\"✅ Using default configuration\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\nCurrent Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Configuration Editor\n",
    "\n",
    "Modify key parameters easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy parameter adjustment\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# config[\"model\"][\"name\"] = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# config[\"hyperparameters\"][\"batch_size\"] = 4\n",
    "# config[\"hyperparameters\"][\"num_iterations\"] = 1000\n",
    "# config[\"distributed\"][\"enabled\"] = True\n",
    "\n",
    "print(\"Model:\", config[\"model\"][\"name\"])\n",
    "print(\"Batch size:\", config[\"hyperparameters\"][\"batch_size\"])\n",
    "print(\"Iterations:\", config[\"hyperparameters\"][\"num_iterations\"])\n",
    "print(\"Distributed:\", config[\"distributed\"][\"enabled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = setup_logging(log_level=\"INFO\")\n",
    "\n",
    "# Initialize cache manager\n",
    "cache_manager = ModelCacheManager()\n",
    "print(f\"Cache directory: {cache_manager.cache_dir}\")\n",
    "\n",
    "# List cached models\n",
    "cached_models = cache_manager.list_cached_models()\n",
    "if cached_models:\n",
    "    print(\"\\nCached models:\")\n",
    "    for model in cached_models:\n",
    "        print(f\"  - {model}\")\n",
    "else:\n",
    "    print(\"\\nNo cached models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation\n",
    "\n",
    "Prepare your dataset for training. You can either:\n",
    "1. Use an existing dataset\n",
    "2. Create a sample dataset for testing\n",
    "3. Load and format your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "dataset_path = Path(config[\"dataset\"][\"path\"])\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"Dataset not found at {dataset_path}\")\n",
    "    print(\"Creating sample dataset for testing...\")\n",
    "    \n",
    "    # Create directory if needed\n",
    "    dataset_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create sample dataset\n",
    "    create_sample_dataset(dataset_path, num_examples=200)\n",
    "\n",
    "# Prepare dataset (format and split)\n",
    "print(\"\\nPreparing dataset...\")\n",
    "dataset_info = prepare_dataset(\n",
    "    file_path=dataset_path,\n",
    "    output_dir=config[\"dataset\"][\"output_dir\"],\n",
    "    val_ratio=config[\"dataset\"][\"validation_split\"],\n",
    "    format_type=config[\"dataset\"][\"format_type\"],\n",
    "    shuffle=config[\"dataset\"].get(\"shuffle\", True),\n",
    "    seed=config[\"dataset\"].get(\"seed\", 42)\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset prepared:\")\n",
    "print(f\"  Training examples: {dataset_info['train_size']}\")\n",
    "print(f\"  Validation examples: {dataset_info['valid_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a few examples\n",
    "train_data = load_jsonl(dataset_info['train'])\n",
    "\n",
    "print(\"Sample training examples:\")\n",
    "for i, example in enumerate(train_data[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    if \"messages\" in example:\n",
    "        for msg in example[\"messages\"]:\n",
    "            print(f\"  {msg['role']}: {msg['content'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "Configure training parameters and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training configuration\n",
    "training_config = {\n",
    "    \"fine_tune_type\": config[\"training\"][\"type\"],\n",
    "    \"num_layers\": config[\"training\"][\"lora\"][\"num_layers\"],\n",
    "    \"lora_parameters\": config[\"training\"][\"lora\"],\n",
    "    \"batch_size\": config[\"hyperparameters\"][\"batch_size\"],\n",
    "    \"iters\": config[\"hyperparameters\"][\"num_iterations\"],\n",
    "    \"learning_rate\": config[\"hyperparameters\"][\"learning_rate\"],\n",
    "    \"val_batches\": config[\"hyperparameters\"][\"val_batches\"],\n",
    "    \"steps_per_report\": config[\"hyperparameters\"][\"steps_per_report\"],\n",
    "    \"steps_per_eval\": config[\"hyperparameters\"][\"steps_per_eval\"],\n",
    "    \"save_every\": config[\"checkpointing\"][\"save_every\"],\n",
    "    \"adapter_path\": config[\"checkpointing\"][\"output_dir\"],\n",
    "    \"max_seq_length\": config[\"dataset\"][\"max_seq_length\"],\n",
    "    \"grad_checkpoint\": config.get(\"optimization\", {}).get(\"gradient_checkpointing\", False)\n",
    "}\n",
    "\n",
    "print(\"Training configuration ready!\")\n",
    "print(f\"Training type: {training_config['fine_tune_type']}\")\n",
    "print(f\"Iterations: {training_config['iters']}\")\n",
    "print(f\"Learning rate: {training_config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single-Node Training\n",
    "\n",
    "Train the model on a single node. This is the simplest way to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (single node)\n",
    "print(\"Starting single-node training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run training\n",
    "results = train_model(\n",
    "    model_name=config[\"model\"][\"name\"],\n",
    "    data_path=config[\"dataset\"][\"output_dir\"],\n",
    "    training_config=training_config,\n",
    "    distributed=False\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "if results.get('success'):\n",
    "    print(f\"✅ Training successful!\")\n",
    "    print(f\"Adapter saved to: {results.get('adapter_path')}\")\n",
    "    if 'final_train_loss' in results:\n",
    "        print(f\"Final training loss: {results['final_train_loss']:.4f}\")\n",
    "    if 'final_val_loss' in results:\n",
    "        print(f\"Final validation loss: {results['final_val_loss']:.4f}\")\n",
    "else:\n",
    "    print(f\"❌ Training failed: {results.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves if available\n",
    "if results.get('success') and 'train_losses' in results:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training loss\n",
    "    if results['train_losses']:\n",
    "        train_iters, train_losses = zip(*results['train_losses'])\n",
    "        plt.plot(train_iters, train_losses, 'b-', label='Training Loss', alpha=0.7)\n",
    "    \n",
    "    # Plot validation loss\n",
    "    if results.get('val_losses'):\n",
    "        val_iters, val_losses = zip(*results['val_losses'])\n",
    "        plt.plot(val_iters, val_losses, 'r-', label='Validation Loss', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distributed Training\n",
    "\n",
    "For distributed training, you need to:\n",
    "1. Set up MPI on your nodes\n",
    "2. Configure the hostfile\n",
    "3. Run this notebook using `mpirun`\n",
    "\n",
    "Example command:\n",
    "```bash\n",
    "mpirun --hostfile hosts.json -np 2 jupyter notebook distributed_training.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in a distributed environment\n",
    "world, size, rank = setup_distributed()\n",
    "\n",
    "if size > 1:\n",
    "    print(f\"🌐 Distributed mode detected!\")\n",
    "    print(f\"World size: {size}\")\n",
    "    print(f\"Current rank: {rank}\")\n",
    "else:\n",
    "    print(\"💻 Single node mode\")\n",
    "    print(\"To use distributed training, run this notebook with mpirun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training (only run if you want to test distributed mode)\n",
    "if config[\"distributed\"][\"enabled\"] and size > 1:\n",
    "    print(\"Starting distributed training...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create distributed callback\n",
    "    dist_callback = DistributedTrainingCallback(world_size=size, rank=rank)\n",
    "    \n",
    "    # Run distributed training\n",
    "    dist_results = train_model(\n",
    "        model_name=config[\"model\"][\"name\"],\n",
    "        data_path=config[\"dataset\"][\"output_dir\"],\n",
    "        training_config=training_config,\n",
    "        distributed=True,\n",
    "        callback=dist_callback\n",
    "    )\n",
    "    \n",
    "    dist_training_time = time.time() - start_time\n",
    "    \n",
    "    if rank == 0:  # Only print from rank 0\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Distributed training completed in {dist_training_time:.2f} seconds\")\n",
    "        print(f\"Speedup: {training_time / dist_training_time:.2f}x\")\n",
    "else:\n",
    "    print(\"Distributed training not enabled or not in distributed environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Test the trained model with some prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = config.get(\"evaluation\", {}).get(\"test_prompts\", [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"How does photosynthesis work?\"\n",
    "])\n",
    "\n",
    "# Evaluate with base model first\n",
    "print(\"=== Base Model Responses ===\")\n",
    "for prompt in test_prompts[:1]:  # Test with first prompt\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = evaluate_model(\n",
    "        model_name=config[\"model\"][\"name\"],\n",
    "        test_prompt=prompt,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with fine-tuned model\n",
    "if results.get('success'):\n",
    "    print(\"\\n=== Fine-tuned Model Responses ===\")\n",
    "    adapter_path = results.get('adapter_path')\n",
    "    \n",
    "    for prompt in test_prompts[:1]:  # Test with first prompt\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        response = evaluate_model(\n",
    "            model_name=config[\"model\"][\"name\"],\n",
    "            adapter_path=adapter_path,\n",
    "            test_prompt=prompt,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(f\"Response: {response}\")\n",
    "else:\n",
    "    print(\"No trained model available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Export Results\n",
    "\n",
    "Save training results and configuration for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if results.get('success'):\n",
    "    results_file = Path(\"training_results.json\")\n",
    "    \n",
    "    # Prepare results for saving\n",
    "    save_results = {\n",
    "        \"model\": config[\"model\"][\"name\"],\n",
    "        \"training_time\": training_time,\n",
    "        \"final_train_loss\": results.get('final_train_loss'),\n",
    "        \"final_val_loss\": results.get('final_val_loss'),\n",
    "        \"adapter_path\": str(results.get('adapter_path')),\n",
    "        \"config\": config,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(save_results, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Results saved to {results_file}\")\n",
    "    print(f\"✅ Adapter saved to {results.get('adapter_path')}\")\n",
    "    print(\"\\nYou can now use the adapter for inference with:\")\n",
    "    print(f\"mlx_lm.generate --model {config['model']['name']} --adapter-path {results.get('adapter_path')} --prompt 'Your prompt here'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tips and Troubleshooting\n",
    "\n",
    "### Memory Issues\n",
    "- Reduce `batch_size` if you run out of memory\n",
    "- Enable `gradient_checkpointing` in the config\n",
    "- Use a smaller model or reduce `max_seq_length`\n",
    "\n",
    "### Distributed Training\n",
    "- Ensure all nodes have the same code and data\n",
    "- Check MPI is properly configured with `mpirun -np 2 hostname`\n",
    "- Use Thunderbolt for best performance between Macs\n",
    "\n",
    "### Dataset Format\n",
    "- Use the `prepare_dataset` function to format your data\n",
    "- Ensure your data has the correct keys (instruction, response, etc.)\n",
    "- Validate your dataset with `validate_dataset`\n",
    "\n",
    "### Next Steps\n",
    "1. Try different models from Hugging Face\n",
    "2. Experiment with hyperparameters\n",
    "3. Create custom datasets for your use case\n",
    "4. Scale up with distributed training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
